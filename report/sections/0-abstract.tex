\begin{abstract}

As reinforcement learning continues to gain widespread application, we aim to validate its foundational algorithms and assess their effectiveness 
within a custom-designed environment. 
Specifically, we evaluate the performance of Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) in the Tetris environment, 
analyzing optimization through ablation experiments that systematically tune hyperparameters. 
To further investigate the generalizability of these methods, we introduce a 3D Cube-Tetris setting, allowing us to examine their 
accuracy and robustness under more complex dynamic conditions.

\end{abstract}