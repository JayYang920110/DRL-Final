\begin{abstract}

As reinforcement learning continues to see widespread adoption, its subfields are rapidly burgeoning, often to the point of overwhelming beginners who struggle to identify an entry point. 
To bridge this gap, we systematically explore the foundational algorithms, evaluating their effectiveness within a custom-designed environment. 
Specifically, we assess the performance of Deep Q-Network (DQN) in the Tetris domain, employing ablation experiments to fine-tune hyperparameters and optimize learning dynamics. 
Additionally, we introduce the \emph{2D-Pentominoes} setting and the \emph{3D-Cube-Tetris} environment, enabling a comprehensive examination of accuracy and robustness under increasingly complex dynamic conditions.

\end{abstract}