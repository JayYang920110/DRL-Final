
=================== Tzu-Heng =========================
@Article{Schrittwieser2020,
author={Schrittwieser, Julian
and Antonoglou, Ioannis
and Hubert, Thomas
and Simonyan, Karen
and Sifre, Laurent
and Schmitt, Simon
and Guez, Arthur
and Lockhart, Edward
and Hassabis, Demis
and Graepel, Thore
and Lillicrap, Timothy
and Silver, David},
title={Mastering Atari, Go, chess and shogi by planning with a learned model},
journal={Nature},
year={2020},
month={Dec},
day={01},
volume={588},
number={7839},
pages={604-609},
abstract={Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
issn={1476-4687},
doi={10.1038/s41586-020-03051-4},
url={https://doi.org/10.1038/s41586-020-03051-4}
}

@misc{mnih2013playingatarideepreinforcement,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5602}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{silver2017masteringchessshogiselfplay,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1712.01815}, 
}

@ARTICLE{5740585,
  author={Yen, Shi-Jim and Yang, Jung-Kuei},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={Two-Stage Monte Carlo Tree Search for Connect6}, 
  year={2011},
  volume={3},
  number={2},
  pages={100-118},
  keywords={Games;Vegetation;Search methods;Law;Monte Carlo methods;Iterative algorithm;Board games;Connect6;conservative TSS;iterative TSS;Monte Carlo tree search;threat space search},
  doi={10.1109/TCIAIG.2011.2134097}}

@PhdThesis{Yang2020,
author={Yang, Tzu-Yee},
editor={Wu, I-Chen},
title={An AlphaZero-Based Design for Connect6},
series={PQDT - Global},
year={2020},
publisher={National Yang Ming Chiao Tung University},
address={Taiwan R.O.C.},
pages={48},
keywords={AlphaZero; 六子棋; Connect6; NCTU6 program; International conferences; Copyright; Computer {\&} video games; Metadata; Preprints; Neural networks; Computer science; Information technology; 0489:Information Technology; 0984:Computer science},
abstract={This thesis applies an AlphaZero-­like algorithm to the game Connect6. In this thesis, we refer galvanise{\_}zero's work and implement a design for AlphaZero connect6. We address the training effectiveness about this approach. We try to remedy a problem, called ''border problem'', we encountered while training. We discovered a special evaluation method that is stronger than the original approach. Finally, We re­train on some different kind of model-­architectures to find out which kind of model-­architectures are more suitable for Connect6. This program achieves a win rate of 74{\%} against the NCTU6 program, which was developed by CGILab earlier. 替代摘要:本論文以 AlphaZero 演算法訓練出一款六子棋程式。在本論文中，我們參考galvanise{\_}zero 專案的作法，修改六子棋規則以應用 AlphaZero 演算法於六子棋之上，並分析此方法的訓練成效。除此之外，我們提出一系列增強我們程式強度的方法，解決我們的程式遭遇到的邊界問題，並提出一種在真實對戰下強度較高的下棋方式。最後，我們透過調整 ResNet 模型架構之寬度與深度以找出適合訓練六子棋的架構。我們讓這支程式與實驗室先前開發的非 AlphaZero 六子棋程式---NCTU6，進行對戰並獲得 74{\%} 勝率。除此之外，這支程式也獲得了由台灣電腦對局協會舉辦之六子棋項目的銀牌。我們亦讓這支程式參與棋類對戰網頁平台 LittleGolem 上第 30 屆六子棋年賽與 2020 年度的月賽，並取得當次年賽冠軍與總計 10 次月賽冠軍。},
note={著作權 - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
note={最後更新 - 2024-11-06},
note={M.C.S.},
note={31596987},
isbn={9798383639450},
url={https://www.proquest.com/dissertations-theses/alphazero-based-design-connect6/docview/3123634451/se-2?accountid=14229},
language={Chinese}
}

﻿@Article{Silver2016,
author={Silver, David
and Huang, Aja
and Maddison, Chris J.
and Guez, Arthur
and Sifre, Laurent
and van den Driessche, George
and Schrittwieser, Julian
and Antonoglou, Ioannis
and Panneershelvam, Veda
and Lanctot, Marc
and Dieleman, Sander
and Grewe, Dominik
and Nham, John
and Kalchbrenner, Nal
and Sutskever, Ilya
and Lillicrap, Timothy
and Leach, Madeleine
and Kavukcuoglu, Koray
and Graepel, Thore
and Hassabis, Demis},
title={Mastering the game of Go with deep neural networks and tree search},
journal={Nature},
year={2016},
month={Jan},
day={01},
volume={529},
number={7587},
pages={484-489},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
issn={1476-4687},
doi={10.1038/nature16961},
url={https://doi.org/10.1038/nature16961}
}

@misc{lillicrap2019continuouscontroldeepreinforcement,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.02971}, 
}

@misc{algorta2019gametetrismachinelearning,
      title={The Game of Tetris in Machine Learning}, 
      author={Simón Algorta and Özgür Şimşek},
      year={2019},
      eprint={1905.01652},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.01652}, 
}

@phdthesis{chen2021playing,
  title={Playing Tetris with deep reinforcement learning},
  author={Chen, Ziao},
  year={2021},
  school={University of Illinois at Urbana-Champaign}
}

@misc{katzgraber2011introductionmontecarlomethods,
      title={Introduction to Monte Carlo Methods}, 
      author={Helmut G. Katzgraber},
      year={2011},
      eprint={0905.1629},
      archivePrefix={arXiv},
      primaryClass={cond-mat.stat-mech},
      url={https://arxiv.org/abs/0905.1629}, 
}

@article{Metropolis01091949,
author = {Nicholas Metropolis and S. Ulam and},
title = {The Monte Carlo Method},
journal = {Journal of the American Statistical Association},
volume = {44},
number = {247},
pages = {335--341},
year = {1949},
publisher = {ASA Website},
doi = {10.1080/01621459.1949.10483310},
note ={PMID: 18139350},
URL = { 
   https://www.tandfonline.com/doi/abs/10.1080/01621459.1949.10483310
    
},
eprint = { 
     https://www.tandfonline.com/doi/pdf/10.1080/01621459.1949.10483310
    
}
}
@inproceedings{10.5555/3022539.3022579,
author = {Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
title = {Monte-carlo tree search: a new framework for game AI},
year = {2008},
publisher = {AAAI Press},
abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, unified framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
booktitle = {Proceedings of the Fourth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {216–217},
numpages = {2},
location = {Stanford, California},
series = {AIIDE'08}
}

@InProceedings{10.1007/11871842_29,
author="Kocsis, Levente
and Szepesv{\'a}ri, Csaba",
editor="F{\"u}rnkranz, Johannes
and Scheffer, Tobias
and Spiliopoulou, Myra",
title="Bandit Based Monte-Carlo Planning",
booktitle="Machine Learning: ECML 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="282--293",
abstract="For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.",
isbn="978-3-540-46056-5"
}
@misc{pathak2017curiositydrivenexplorationselfsupervisedprediction,
      title={Curiosity-driven Exploration by Self-supervised Prediction}, 
      author={Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
      year={2017},
      eprint={1705.05363},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1705.05363}, 
}

@misc{badia2020uplearningdirectedexploration,
      title={Never Give Up: Learning Directed Exploration Strategies}, 
      author={Adrià Puigdomènech Badia and Pablo Sprechmann and Alex Vitvitskyi and Daniel Guo and Bilal Piot and Steven Kapturowski and Olivier Tieleman and Martín Arjovsky and Alexander Pritzel and Andew Bolt and Charles Blundell},
      year={2020},
      eprint={2002.06038},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.06038}, 
}